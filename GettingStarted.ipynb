{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2af0007",
   "metadata": {},
   "source": [
    "## Reference Implementation\n",
    "\n",
    "### E2E Architecture\n",
    "\n",
    "![use_case_flow](assets/conversationai-e2e-flow.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89cbc5c",
   "metadata": {},
   "source": [
    "### Solution setup\n",
    "Use the following cell to change to the correct kernel. Then check that you are in the `stock` kernel. If not, navigate to `Kernel > Change kernel > Python [conda env:stock]`. Note that the cell will remain with * but you can continue running the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5e7367",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-customer_chatbot_intel-py'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c649ea71",
   "metadata": {},
   "source": [
    "Instructions to download and prepare this dataset for benchmarking using these scripts can be found by in the `data` directory [here](data/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f324b15c",
   "metadata": {},
   "source": [
    "### Running the Benchmarks for Training\n",
    "\n",
    "Benchmarking for training can be done using the python script `run_training.py`.\n",
    "\n",
    "The script *reads and preprocesses the data*, *trains an joint clasification and entity recognition model*, and *predicts on unseen test data* using the trained model, while also reporting on the execution time for these 3 steps.  ***Optionally, the script can also save the trained model weights, which is necessary to run the inference benchmarks***.\n",
    "\n",
    "The run benchmark script takes the following arguments:\n",
    "\n",
    "```shell\n",
    "usage: run_training.py [-h] [-l LOGFILE] [-i] [-s SAVE_MODEL_DIR] [--save_onnx]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -l LOGFILE, --logfile LOGFILE\n",
    "                        log file to output benchmarking results to\n",
    "  -i, --intel           use intel accelerated technologies where available\n",
    "  -s SAVE_MODEL_DIR, --save_model_dir SAVE_MODEL_DIR\n",
    "                        directory to save model under\n",
    "  --save_onnx           also export an ONNX model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c4783",
   "metadata": {},
   "source": [
    "#### Training the Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ac0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m intel_extension_for_pytorch.cpu.launch $WORKSPACE/src/run_training.py --logfile $OUTPUT_DIR/logs/intel.log -s $DATA_DIR/saved_models/intel -d $DATA_DIR/atis-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302db2a1",
   "metadata": {},
   "source": [
    "### Running the Benchmarks for Inference\n",
    "\n",
    "Benchmarking for inference for Pytorch (.pt) models can be done using the python script `run_inference.py`.\n",
    "\n",
    "`run_inference.py` : runs inference benchmarks using models optimized using the Intel® Extension for PyTorch*.\n",
    "\n",
    "The `run_inference.py` script takes the following arguments:\n",
    "\n",
    "```shell\n",
    "usage: run_inference.py [-h] -s SAVED_MODEL_DIR [--is_jit] [--is_inc_int8] [-i] [-b BATCH_SIZE] [-l LENGTH]\n",
    "                        [--logfile LOGFILE] [-n N_RUNS]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -s SAVED_MODEL_DIR, --saved_model_dir SAVED_MODEL_DIR\n",
    "                        directory of saved model to benchmark.\n",
    "  --is_jit              if the model is torchscript. defaults to False.\n",
    "  --is_inc_int8         saved model dir is a quantized int8 model. defaults to False.\n",
    "  -i, --intel           use intel accelerated technologies. defaults to False.\n",
    "  -b BATCH_SIZE, --batch_size BATCH_SIZE\n",
    "                        batch size to use. defaults to 200.\n",
    "  -l LENGTH, --length LENGTH\n",
    "                        sequence length to use. defaults to 512.\n",
    "  --logfile LOGFILE     logfile to use.\n",
    "  -n N_RUNS, --n_runs N_RUNS\n",
    "                        number of trials to test. defaults to 100.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c5977",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m intel_extension_for_pytorch.cpu.launch $WORKSPACE/src/run_inference.py -s $DATA_DIR/saved_models/intel --batch_size 200 --length 512 --n_runs 5 --logfile $OUTPUT_DIR/logs/intel.log -d $DATA_DIR/atis-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b34223",
   "metadata": {},
   "source": [
    "#### Intel® Neural Compressor Quantization\n",
    "\n",
    "A trained model from the `run_training.py` script above can be quantized \n",
    "using [Intel® Neural Compressor](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) \n",
    "through the `run_quantize_inc.py` script.  This converts the model from FP32 to INT8 while trying to \n",
    "maintain a specified level of accuracy specified via a `config.yaml` file. A simple `config.yaml` has been \n",
    "provided for basic accuracy aware quantization though several further options exist and can be explored in the link above.\n",
    "\n",
    "\n",
    "```shell\n",
    "usage: run_quantize_inc.py [-h] -s SAVED_MODEL -o OUTPUT_DIR [-l LENGTH] [-q QUANT_SAMPLES] -c INC_CONFIG\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -s SAVED_MODEL, --saved_model SAVED_MODEL\n",
    "                        saved pytorch (.pt) model to quantize.\n",
    "  -o OUTPUT_DIR, --output_dir OUTPUT_DIR\n",
    "                        directory to save quantized model to.\n",
    "  -l LENGTH, --length LENGTH\n",
    "                        sequence length to use. defaults to 512.\n",
    "  -q QUANT_SAMPLES, --quant_samples QUANT_SAMPLES\n",
    "                        number of samples to use for quantization. defaults to 100.\n",
    "  -c INC_CONFIG, --inc_config INC_CONFIG\n",
    "                        INC conf yaml.\n",
    "```\n",
    "\n",
    "A workflow of training -> INC quantization -> inference benchmarking may look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aaf43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "python $WORKSPACE/src/run_quantize_inc.py -s $DATA_DIR/saved_models/intel/convai.pt -o $DATA_DIR/saved_models/intel_int8/ -c $CONFIG_DIR/config.yml -d $DATA_DIR/atis-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc3a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m intel_extension_for_pytorch.cpu.launch $WORKSPACE/src/run_inference.py -s $DATA_DIR/saved_models/intel_int8/ -b 1 -n 1000 --is_inc_int8 --logfile $OUTPUT_DIR/logs/intel.log -d $DATA_DIR/atis-2/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:customer_chatbot_intel] *",
   "language": "python",
   "name": "conda-env-customer_chatbot_intel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "47ddfd05091c518f1457bda86c2e52f2367741cdbeac658212cf94c78cf8a125"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
